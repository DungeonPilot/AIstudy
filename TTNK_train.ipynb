{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "DATASETSIZE = 800\n",
    "LEARNING_RATE_BASE = 0.99\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.08#正则化率\n",
    "TRAINING_STEPS = 10000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "PURE_LOSS_WIGHT = 0.5\n",
    "L1_OVER_L1ADDL2 = 0.5\n",
    "# 模型保存的路径和文件名\n",
    "MODEL_SAVE_PATH = \"TTNKmodels\"\n",
    "MODEL_NAME = \"model.ckpt\"\n",
    "\n",
    "# 定义神经网路相关参数|\n",
    "INPUT_NODE = 33\n",
    "OUTPUT_NODE = 2\n",
    "LAYER1_NODE = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'TTNKdata'\n",
    "_train_path=os.path.join(folder,'prepressed_train_data.csv')\n",
    "train_data=pd.read_csv(_train_path,header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2     3    4    5        6    7    8    9  ...    25   26   27  \\\n",
       "0  1.0  3.0  1.0  22.0  1.0  0.0   7.2500  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       "1  2.0  1.0  0.0  38.0  1.0  0.0  71.2833  0.0  0.0  0.0 ...   0.0  1.0  0.0   \n",
       "2  3.0  3.0  0.0  26.0  0.0  0.0   7.9250  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       "3  4.0  1.0  0.0  35.0  1.0  0.0  53.1000  0.0  0.0  0.0 ...   0.0  1.0  0.0   \n",
       "4  5.0  3.0  1.0  35.0  0.0  0.0   8.0500  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
       "\n",
       "    28   29   30   31   32     33   34  \n",
       "0  0.0  0.0  0.0  0.0  1.0    0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0   85.0  1.0  \n",
       "2  0.0  0.0  0.0  0.0  1.0    0.0  1.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  123.0  1.0  \n",
       "4  0.0  0.0  0.0  0.0  1.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 35)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0]]\n",
      "(800, 33)\n",
      "[ 1.      0.     38.      1.      0.     71.2833  0.      0.      0.\n",
      "  0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "  0.      1.      0.      0.      0.      0.      0.      1.      0.\n",
      "  0.      0.      0.      0.      0.     85.    ]\n"
     ]
    }
   ],
   "source": [
    "#X Y 分别为属性和标签\n",
    "train_Y = np.array(train_data)[:DATASETSIZE,34]\n",
    "Y = []\n",
    "for y in train_Y:\n",
    "    if y==0:\n",
    "        Y.append([1,0])\n",
    "    else:\n",
    "        Y.append([0,1])\n",
    "print(Y)\n",
    "X = np.array(train_data)[:DATASETSIZE,1:34]\n",
    "X = X.astype(np.float32)\n",
    "    #numpy默认是float64，跟神经网络定义的时候的变量类型(float32)不符合，会出点问题\n",
    "    #使用astype()改变类型\n",
    "print(X.shape)\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "(91, 33)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "#构造测试集\n",
    "val_y = np.array(train_data)[DATASETSIZE:,34]\n",
    "val_Y = []\n",
    "for y in val_y:\n",
    "    if y==0:\n",
    "        val_Y.append([1,0])\n",
    "    else:\n",
    "        val_Y.append([0,1])\n",
    "val_Y = np.array(val_Y)\n",
    "print(val_Y)\n",
    "val_X = np.array(train_data)[DATASETSIZE:,1:34]\n",
    "val_X = val_X.astype(np.float32)\n",
    "    #numpy默认是float64，跟神经网络定义的时候的变量类型(float32)不符合，会出点问题\n",
    "    #使用astype()改变类型\n",
    "print(val_X.shape)\n",
    "print(type(X))\n",
    "print(type(val_X[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取变量\n",
    "def get_weights_variable(shape, regularizer):\n",
    "    weights = tf.get_variable(\n",
    "        \"weights\", shape,dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "    # 当给出了正则化函数时，将当前变量的正则化损失加入名字为losses的集合当中\n",
    "    # 这个集合不再TF自动管理的集合列表中\n",
    "    if regularizer != None:\n",
    "        tf.add_to_collection('losses', regularizer(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网路的向前传播过程\n",
    "def inference(input_tensor, regularizer):  # regulaizer 正则化\n",
    "    # 声明第一层神经网路的变量并完成向前传播过程\n",
    "    with tf.variable_scope(\"layer1\",reuse=tf.AUTO_REUSE):\n",
    "        #  如果存在多次调用该函数，需要在第一次调用后将reuse设置为True\n",
    "        weights = get_weights_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases) #Acc on val is 0.857143\n",
    "        #layer1 = tf.nn.leaky_relu(tf.matmul(input_tensor, weights) + biases)#Acc on val is 0.868132\n",
    "        \n",
    "        # 第二层神经网络\n",
    "    with tf.variable_scope(\"layer2\",reuse = tf.AUTO_REUSE):\n",
    "        weights = get_weights_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "    \n",
    "    #使用sotfmax转化为概率值\n",
    "    layer_to_return = tf.nn.softmax(layer2)\n",
    "    # 返回最后的向前传播结果\n",
    "    return layer_to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(val_X,val_Y,sess):\n",
    "    #numpy默认是float64，跟神经网络定义的时候的变量类型(float32)不符合，会出点问题\n",
    "    #使用astype()改变类型\n",
    "    #print(type(val_X)) #<class 'numpy.ndarray'>\n",
    "    local_Y = sess.run(inference(val_X,regularizer=None))\n",
    "    right = 0.0\n",
    "    counts = len(val_Y)\n",
    "    for local_y,val_y in zip(local_Y,val_Y):\n",
    "        if local_y[0]>=local_y[1] and val_y[0]==1:\n",
    "            right+=1\n",
    "        elif local_y[0]<local_y[1] and val_y[0]==0:\n",
    "            right+=1\n",
    "        else:\n",
    "            pass\n",
    "    return right/counts\n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(val_Y, 1), tf.argmax(local_Y, 1))\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  # casts a tensor to a new type\n",
    "    \n",
    "#     return sess.run(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data):\n",
    "    X, Y = train_data[0],train_data[1]\n",
    "    # 定义输入输出placeholder\n",
    "    x = tf.placeholder(tf.float32, [None,INPUT_NODE], name=\"x-input\")\n",
    "    y_ = tf.placeholder(tf.float32, [None,OUTPUT_NODE], name=\"y-input\")\n",
    "    regularizer = tf.contrib.layers.l1_l2_regularizer(REGULARAZTION_RATE*L1_OVER_L1ADDL2,REGULARAZTION_RATE*(1-L1_OVER_L1ADDL2))#希望矩阵更稀疏\n",
    "#     Args:\n",
    "#       scale_l1: A scalar multiplier `Tensor` for L1 regularization.\n",
    "#       scale_l2: A scalar multiplier `Tensor` for L2 regularization.\n",
    "#       scope: An optional scope name.\n",
    "\n",
    "    # 直接使用_inference中的向前传播过程\n",
    "    y = inference(x, regularizer)\n",
    "    global_steps = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数、学习率、滑动平均操作及训练过程\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_steps)\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    try:\n",
    "        variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #不均衡的数据集合，使用自定义损失\n",
    "    #pure_loss = tf.reduce_sum()\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_sum = tf.reduce_sum(cross_entropy)#改成reducesum后好了很多\n",
    "    loss = cross_entropy_sum*PURE_LOSS_WIGHT + tf.add_n(tf.get_collection('losses'))*(1-PURE_LOSS_WIGHT)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_steps,\n",
    "        len(X) / BATCH_SIZE*10,\n",
    "        LEARNING_RATE_DECAY)\n",
    "    \n",
    "    train_step = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_steps)\n",
    "    #train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n",
    "       # .minimize(loss, global_step=global_steps)#更换优化器\n",
    "    # 下面是一次性完成两个op的操作\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 初始化TF持久化类\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        train_writer = tf.summary.FileWriter('TTNKlogs/',sess.graph)#定义一个写入summary的目标文件，dir为写入文件地址  \n",
    "        tf.summary.scalar('loss',loss) #loss标量的图\n",
    "        merge_summary = tf.summary.merge_all()\n",
    "        #merge_summary = tf.summary.merge([tf.get_collection(tf.GraphKeys.SUMMARIES,'losses')])  #使用tf.get_collection函数筛选图中summary信息中的accuracy信\n",
    "        # 这里只有训练过程，没有验证和测试过程\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            start = (i*BATCH_SIZE)%DATASETSIZE\n",
    "            end = min(start+BATCH_SIZE,DATASETSIZE)\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_steps], feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "            # 每1000轮保存一次模型\n",
    "            if True:\n",
    "                train_summary = sess.run(merge_summary,feed_dict={x:X[start:end],y_:Y[start:end]})#调用sess.run运行图，生成一步的训练过程数据  \n",
    "                train_writer.add_summary(train_summary,step)#调用train_writer的add_summary方法将训练过程以及训练步数保存  \n",
    "            if i % 1000 == 0:\n",
    "                # 输出当前轮的训练情况，这里是输出当前batch的损失函数的大小。\n",
    "                # 正确率会由一个单独的程序来生成\n",
    "                print (\"After %d training step(s), loss on training \"\n",
    "                       \"batch is %g.\" % (step, loss_value))\n",
    "                pure_loss = sess.run(cross_entropy_sum,feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "                print(\"After %d training step(s), pure_loss on training \"\n",
    "                       \"batch is %g.\" % (step, pure_loss))\n",
    "                print(\"After %d training step(s), regulayzer on training \"\n",
    "                       \"batch is %g.\" % (step,loss_value - pure_loss*PURE_LOSS_WIGHT))\n",
    "                # 保存当前的模型。并在模型名后添加训练的轮数\n",
    "                print(\"Acc on val is %f\"%acc(val_X,val_Y,sess))\n",
    "                print(\"Acc on test is %f\"%acc(X,Y,sess))\n",
    "                saver.save(\n",
    "                    sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME),\n",
    "                    global_step=global_steps)\n",
    "#             if i % 5000 ==0:\n",
    "#                 print(sess.run(inference(X,regularizer=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training step(s), loss on training batch is 78.458.\n",
      "After 1 training step(s), pure_loss on training batch is 90.3212.\n",
      "After 1 training step(s), regulayzer on training batch is 33.2974.\n",
      "Acc on val is 0.373626\n",
      "Acc on test is 0.385000\n",
      "After 1001 training step(s), loss on training batch is 42.006.\n",
      "After 1001 training step(s), pure_loss on training batch is 52.4927.\n",
      "After 1001 training step(s), regulayzer on training batch is 15.7596.\n",
      "Acc on val is 0.813187\n",
      "Acc on test is 0.816250\n",
      "After 2001 training step(s), loss on training batch is 33.382.\n",
      "After 2001 training step(s), pure_loss on training batch is 52.5882.\n",
      "After 2001 training step(s), regulayzer on training batch is 7.08794.\n",
      "Acc on val is 0.813187\n",
      "Acc on test is 0.820000\n",
      "After 3001 training step(s), loss on training batch is 32.0056.\n",
      "After 3001 training step(s), pure_loss on training batch is 52.6772.\n",
      "After 3001 training step(s), regulayzer on training batch is 5.66705.\n",
      "Acc on val is 0.835165\n",
      "Acc on test is 0.825000\n",
      "After 4001 training step(s), loss on training batch is 31.551.\n",
      "After 4001 training step(s), pure_loss on training batch is 52.7063.\n",
      "After 4001 training step(s), regulayzer on training batch is 5.1978.\n",
      "Acc on val is 0.835165\n",
      "Acc on test is 0.826250\n",
      "After 5001 training step(s), loss on training batch is 31.2414.\n",
      "After 5001 training step(s), pure_loss on training batch is 52.6781.\n",
      "After 5001 training step(s), regulayzer on training batch is 4.90236.\n",
      "Acc on val is 0.835165\n",
      "Acc on test is 0.828750\n",
      "After 6001 training step(s), loss on training batch is 30.7884.\n",
      "After 6001 training step(s), pure_loss on training batch is 52.2314.\n",
      "After 6001 training step(s), regulayzer on training batch is 4.67269.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.827500\n",
      "After 7001 training step(s), loss on training batch is 29.5337.\n",
      "After 7001 training step(s), pure_loss on training batch is 49.7369.\n",
      "After 7001 training step(s), regulayzer on training batch is 4.66523.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.830000\n",
      "After 8001 training step(s), loss on training batch is 29.0973.\n",
      "After 8001 training step(s), pure_loss on training batch is 49.3509.\n",
      "After 8001 training step(s), regulayzer on training batch is 4.42184.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.836250\n",
      "After 9001 training step(s), loss on training batch is 29.0198.\n",
      "After 9001 training step(s), pure_loss on training batch is 49.3832.\n",
      "After 9001 training step(s), regulayzer on training batch is 4.32817.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.838750\n",
      "After 10001 training step(s), loss on training batch is 28.9083.\n",
      "After 10001 training step(s), pure_loss on training batch is 49.3386.\n",
      "After 10001 training step(s), regulayzer on training batch is 4.23899.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.840000\n",
      "After 11001 training step(s), loss on training batch is 28.8761.\n",
      "After 11001 training step(s), pure_loss on training batch is 49.429.\n",
      "After 11001 training step(s), regulayzer on training batch is 4.16159.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.843750\n",
      "After 12001 training step(s), loss on training batch is 28.8261.\n",
      "After 12001 training step(s), pure_loss on training batch is 49.4354.\n",
      "After 12001 training step(s), regulayzer on training batch is 4.10839.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.847500\n",
      "After 13001 training step(s), loss on training batch is 28.7506.\n",
      "After 13001 training step(s), pure_loss on training batch is 49.3794.\n",
      "After 13001 training step(s), regulayzer on training batch is 4.06091.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.847500\n",
      "After 14001 training step(s), loss on training batch is 28.6803.\n",
      "After 14001 training step(s), pure_loss on training batch is 49.3282.\n",
      "After 14001 training step(s), regulayzer on training batch is 4.01623.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.848750\n",
      "After 15001 training step(s), loss on training batch is 28.6154.\n",
      "After 15001 training step(s), pure_loss on training batch is 49.2808.\n",
      "After 15001 training step(s), regulayzer on training batch is 3.97502.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.848750\n",
      "After 16001 training step(s), loss on training batch is 28.5436.\n",
      "After 16001 training step(s), pure_loss on training batch is 49.2176.\n",
      "After 16001 training step(s), regulayzer on training batch is 3.93483.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.850000\n",
      "After 17001 training step(s), loss on training batch is 28.4712.\n",
      "After 17001 training step(s), pure_loss on training batch is 49.1543.\n",
      "After 17001 training step(s), regulayzer on training batch is 3.89402.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.853750\n",
      "After 18001 training step(s), loss on training batch is 28.4045.\n",
      "After 18001 training step(s), pure_loss on training batch is 49.0997.\n",
      "After 18001 training step(s), regulayzer on training batch is 3.85465.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.853750\n",
      "After 19001 training step(s), loss on training batch is 28.3431.\n",
      "After 19001 training step(s), pure_loss on training batch is 49.046.\n",
      "After 19001 training step(s), regulayzer on training batch is 3.82011.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.856250\n",
      "After 20001 training step(s), loss on training batch is 28.2962.\n",
      "After 20001 training step(s), pure_loss on training batch is 49.0155.\n",
      "After 20001 training step(s), regulayzer on training batch is 3.78851.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.858750\n",
      "After 21001 training step(s), loss on training batch is 28.2438.\n",
      "After 21001 training step(s), pure_loss on training batch is 48.9704.\n",
      "After 21001 training step(s), regulayzer on training batch is 3.75858.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.860000\n",
      "After 22001 training step(s), loss on training batch is 28.1847.\n",
      "After 22001 training step(s), pure_loss on training batch is 48.9127.\n",
      "After 22001 training step(s), regulayzer on training batch is 3.72834.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.860000\n",
      "After 23001 training step(s), loss on training batch is 28.1275.\n",
      "After 23001 training step(s), pure_loss on training batch is 48.8633.\n",
      "After 23001 training step(s), regulayzer on training batch is 3.69589.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.861250\n",
      "After 24001 training step(s), loss on training batch is 28.0758.\n",
      "After 24001 training step(s), pure_loss on training batch is 48.8191.\n",
      "After 24001 training step(s), regulayzer on training batch is 3.66631.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.861250\n",
      "After 25001 training step(s), loss on training batch is 28.0284.\n",
      "After 25001 training step(s), pure_loss on training batch is 48.7818.\n",
      "After 25001 training step(s), regulayzer on training batch is 3.63754.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.860000\n",
      "After 26001 training step(s), loss on training batch is 27.9852.\n",
      "After 26001 training step(s), pure_loss on training batch is 48.7504.\n",
      "After 26001 training step(s), regulayzer on training batch is 3.61006.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.861250\n",
      "After 27001 training step(s), loss on training batch is 27.9419.\n",
      "After 27001 training step(s), pure_loss on training batch is 48.7164.\n",
      "After 27001 training step(s), regulayzer on training batch is 3.58372.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.861250\n",
      "After 28001 training step(s), loss on training batch is 27.896.\n",
      "After 28001 training step(s), pure_loss on training batch is 48.6762.\n",
      "After 28001 training step(s), regulayzer on training batch is 3.55789.\n",
      "Acc on val is 0.846154\n",
      "Acc on test is 0.861250\n",
      "After 29001 training step(s), loss on training batch is 27.8486.\n",
      "After 29001 training step(s), pure_loss on training batch is 48.6318.\n",
      "After 29001 training step(s), regulayzer on training batch is 3.53274.\n",
      "Acc on val is 0.857143\n",
      "Acc on test is 0.861250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1654bc61a228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-9ce59075cb2d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mDATASETSIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDATASETSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;31m# 每1000轮保存一次模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 初始化计算图\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.999\n",
    "REGULARAZTION_RATE = 0.07#正则化率\n",
    "TRAINING_STEPS = 100000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "PURE_LOSS_WIGHT = 0.5\n",
    "L1_OVER_L1ADDL2 = 0.6\n",
    "\n",
    "try:\n",
    "    tf.reset_default_graph()\n",
    "except:\n",
    "    pass\n",
    "train([X,Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE_BASE = 0.9\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARAZTION_RATE = 0.07#正则化率\n",
    "TRAINING_STEPS = 10000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "PURE_LOSS_WIGHT = 0.5\n",
    "L1_OVER_L1ADDL2 = 0.3\n",
    "# Acc on val is 0.879121\n",
    "# Acc on test is 0.846250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化计算图\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.999\n",
    "REGULARAZTION_RATE = 0.07#正则化率\n",
    "TRAINING_STEPS = 100000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "PURE_LOSS_WIGHT = 0.5\n",
    "L1_OVER_L1ADDL2 = 0.6\n",
    "# Acc on val is 0.868132\n",
    "# Acc on test is 0.872500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0]]\n",
      "(891, 33)\n",
      "[ 1.      0.     38.      1.      0.     71.2833  0.      0.      0.\n",
      "  0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "  0.      1.      0.      0.      0.      0.      0.      1.      0.\n",
      "  0.      0.      0.      0.      0.     85.    ]\n"
     ]
    }
   ],
   "source": [
    "#选定参数后使用全部数据训练：\n",
    "train_Y = np.array(train_data)[:,34]\n",
    "Y = []\n",
    "for y in train_Y:\n",
    "    if y==0:\n",
    "        Y.append([1,0])\n",
    "    else:\n",
    "        Y.append([0,1])\n",
    "print(Y)\n",
    "X = np.array(train_data)[:,1:34]\n",
    "X = X.astype(np.float32)\n",
    "    #numpy默认是float64，跟神经网络定义的时候的变量类型(float32)不符合，会出点问题\n",
    "    #使用astype()改变类型\n",
    "print(X.shape)\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETSIZE = 891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 training step(s), loss on training batch is 68.4857.\n",
      "After 1 training step(s), pure_loss on training batch is 70.4854.\n",
      "After 1 training step(s), regulayzer on training batch is 33.2431.\n",
      "Acc on val is 0.626374\n",
      "Acc on test is 0.616162\n",
      "After 1001 training step(s), loss on training batch is 42.3403.\n",
      "After 1001 training step(s), pure_loss on training batch is 51.9782.\n",
      "After 1001 training step(s), regulayzer on training batch is 16.3512.\n",
      "Acc on val is 0.868132\n",
      "Acc on test is 0.824916\n",
      "After 2001 training step(s), loss on training batch is 33.9729.\n",
      "After 2001 training step(s), pure_loss on training batch is 52.1765.\n",
      "After 2001 training step(s), regulayzer on training batch is 7.88469.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.831650\n",
      "After 3001 training step(s), loss on training batch is 29.5565.\n",
      "After 3001 training step(s), pure_loss on training batch is 46.386.\n",
      "After 3001 training step(s), regulayzer on training batch is 6.36352.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.826038\n",
      "After 4001 training step(s), loss on training batch is 18.4655.\n",
      "After 4001 training step(s), pure_loss on training batch is 25.3612.\n",
      "After 4001 training step(s), regulayzer on training batch is 5.78487.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.833895\n",
      "After 5001 training step(s), loss on training batch is 29.1726.\n",
      "After 5001 training step(s), pure_loss on training batch is 47.626.\n",
      "After 5001 training step(s), regulayzer on training batch is 5.35962.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.840629\n",
      "After 6001 training step(s), loss on training batch is 30.7838.\n",
      "After 6001 training step(s), pure_loss on training batch is 51.22.\n",
      "After 6001 training step(s), regulayzer on training batch is 5.17379.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.839506\n",
      "After 7001 training step(s), loss on training batch is 29.1788.\n",
      "After 7001 training step(s), pure_loss on training batch is 48.1025.\n",
      "After 7001 training step(s), regulayzer on training batch is 5.12756.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.841751\n",
      "After 8001 training step(s), loss on training batch is 27.0016.\n",
      "After 8001 training step(s), pure_loss on training batch is 44.5058.\n",
      "After 8001 training step(s), regulayzer on training batch is 4.74865.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.842873\n",
      "After 9001 training step(s), loss on training batch is 28.5863.\n",
      "After 9001 training step(s), pure_loss on training batch is 46.5622.\n",
      "After 9001 training step(s), regulayzer on training batch is 5.3052.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.850730\n",
      "After 10001 training step(s), loss on training batch is 26.3155.\n",
      "After 10001 training step(s), pure_loss on training batch is 43.2671.\n",
      "After 10001 training step(s), regulayzer on training batch is 4.68195.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.842873\n",
      "After 11001 training step(s), loss on training batch is 28.3995.\n",
      "After 11001 training step(s), pure_loss on training batch is 47.7701.\n",
      "After 11001 training step(s), regulayzer on training batch is 4.51444.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.848485\n",
      "After 12001 training step(s), loss on training batch is 25.4673.\n",
      "After 12001 training step(s), pure_loss on training batch is 41.9006.\n",
      "After 12001 training step(s), regulayzer on training batch is 4.51697.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.848485\n",
      "After 13001 training step(s), loss on training batch is 29.5662.\n",
      "After 13001 training step(s), pure_loss on training batch is 50.0947.\n",
      "After 13001 training step(s), regulayzer on training batch is 4.51888.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.847363\n",
      "After 14001 training step(s), loss on training batch is 29.6566.\n",
      "After 14001 training step(s), pure_loss on training batch is 50.7821.\n",
      "After 14001 training step(s), regulayzer on training batch is 4.26559.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.849607\n",
      "After 15001 training step(s), loss on training batch is 28.0187.\n",
      "After 15001 training step(s), pure_loss on training batch is 47.627.\n",
      "After 15001 training step(s), regulayzer on training batch is 4.20517.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.850730\n",
      "After 16001 training step(s), loss on training batch is 26.4612.\n",
      "After 16001 training step(s), pure_loss on training batch is 44.6032.\n",
      "After 16001 training step(s), regulayzer on training batch is 4.15961.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.856341\n",
      "After 17001 training step(s), loss on training batch is 9.69947.\n",
      "After 17001 training step(s), pure_loss on training batch is 11.1896.\n",
      "After 17001 training step(s), regulayzer on training batch is 4.10468.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.851852\n",
      "After 18001 training step(s), loss on training batch is 29.5969.\n",
      "After 18001 training step(s), pure_loss on training batch is 51.0094.\n",
      "After 18001 training step(s), regulayzer on training batch is 4.09213.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.855219\n",
      "After 19001 training step(s), loss on training batch is 28.9151.\n",
      "After 19001 training step(s), pure_loss on training batch is 49.7406.\n",
      "After 19001 training step(s), regulayzer on training batch is 4.04485.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.851852\n",
      "After 20001 training step(s), loss on training batch is 26.9257.\n",
      "After 20001 training step(s), pure_loss on training batch is 45.6878.\n",
      "After 20001 training step(s), regulayzer on training batch is 4.08182.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.854097\n",
      "After 21001 training step(s), loss on training batch is 23.2703.\n",
      "After 21001 training step(s), pure_loss on training batch is 38.605.\n",
      "After 21001 training step(s), regulayzer on training batch is 3.96777.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.855219\n",
      "After 22001 training step(s), loss on training batch is 27.4171.\n",
      "After 22001 training step(s), pure_loss on training batch is 46.9783.\n",
      "After 22001 training step(s), regulayzer on training batch is 3.92799.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.855219\n",
      "After 23001 training step(s), loss on training batch is 26.221.\n",
      "After 23001 training step(s), pure_loss on training batch is 44.6784.\n",
      "After 23001 training step(s), regulayzer on training batch is 3.88177.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.858586\n",
      "After 24001 training step(s), loss on training batch is 27.2198.\n",
      "After 24001 training step(s), pure_loss on training batch is 46.5871.\n",
      "After 24001 training step(s), regulayzer on training batch is 3.92628.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.856341\n",
      "After 25001 training step(s), loss on training batch is 25.3443.\n",
      "After 25001 training step(s), pure_loss on training batch is 42.9421.\n",
      "After 25001 training step(s), regulayzer on training batch is 3.8733.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.855219\n",
      "After 26001 training step(s), loss on training batch is 27.6937.\n",
      "After 26001 training step(s), pure_loss on training batch is 47.3075.\n",
      "After 26001 training step(s), regulayzer on training batch is 4.03993.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.856341\n",
      "After 27001 training step(s), loss on training batch is 27.0632.\n",
      "After 27001 training step(s), pure_loss on training batch is 46.547.\n",
      "After 27001 training step(s), regulayzer on training batch is 3.78975.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.857464\n",
      "After 28001 training step(s), loss on training batch is 27.4319.\n",
      "After 28001 training step(s), pure_loss on training batch is 47.4053.\n",
      "After 28001 training step(s), regulayzer on training batch is 3.72926.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.856341\n",
      "After 29001 training step(s), loss on training batch is 25.1102.\n",
      "After 29001 training step(s), pure_loss on training batch is 42.7777.\n",
      "After 29001 training step(s), regulayzer on training batch is 3.72136.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.855219\n",
      "After 30001 training step(s), loss on training batch is 29.2229.\n",
      "After 30001 training step(s), pure_loss on training batch is 50.8725.\n",
      "After 30001 training step(s), regulayzer on training batch is 3.78663.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.858586\n",
      "After 31001 training step(s), loss on training batch is 29.1067.\n",
      "After 31001 training step(s), pure_loss on training batch is 50.7946.\n",
      "After 31001 training step(s), regulayzer on training batch is 3.70946.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.858586\n",
      "After 32001 training step(s), loss on training batch is 28.5131.\n",
      "After 32001 training step(s), pure_loss on training batch is 49.7.\n",
      "After 32001 training step(s), regulayzer on training batch is 3.66307.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.858586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 33001 training step(s), loss on training batch is 26.1603.\n",
      "After 33001 training step(s), pure_loss on training batch is 44.8759.\n",
      "After 33001 training step(s), regulayzer on training batch is 3.72237.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.860831\n",
      "After 34001 training step(s), loss on training batch is 15.5282.\n",
      "After 34001 training step(s), pure_loss on training batch is 23.7909.\n",
      "After 34001 training step(s), regulayzer on training batch is 3.63278.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.859708\n",
      "After 35001 training step(s), loss on training batch is 27.0072.\n",
      "After 35001 training step(s), pure_loss on training batch is 46.8508.\n",
      "After 35001 training step(s), regulayzer on training batch is 3.5818.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.859708\n",
      "After 36001 training step(s), loss on training batch is 27.3488.\n",
      "After 36001 training step(s), pure_loss on training batch is 47.5225.\n",
      "After 36001 training step(s), regulayzer on training batch is 3.58754.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.859708\n",
      "After 37001 training step(s), loss on training batch is 27.0133.\n",
      "After 37001 training step(s), pure_loss on training batch is 46.6712.\n",
      "After 37001 training step(s), regulayzer on training batch is 3.67772.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.859708\n",
      "After 38001 training step(s), loss on training batch is 24.9161.\n",
      "After 38001 training step(s), pure_loss on training batch is 42.7878.\n",
      "After 38001 training step(s), regulayzer on training batch is 3.52221.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.859708\n",
      "After 39001 training step(s), loss on training batch is 26.3955.\n",
      "After 39001 training step(s), pure_loss on training batch is 45.1799.\n",
      "After 39001 training step(s), regulayzer on training batch is 3.80553.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.859708\n",
      "After 40001 training step(s), loss on training batch is 24.655.\n",
      "After 40001 training step(s), pure_loss on training batch is 42.298.\n",
      "After 40001 training step(s), regulayzer on training batch is 3.50601.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.861953\n",
      "After 41001 training step(s), loss on training batch is 26.709.\n",
      "After 41001 training step(s), pure_loss on training batch is 46.399.\n",
      "After 41001 training step(s), regulayzer on training batch is 3.50949.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.859708\n",
      "After 42001 training step(s), loss on training batch is 24.0577.\n",
      "After 42001 training step(s), pure_loss on training batch is 41.0687.\n",
      "After 42001 training step(s), regulayzer on training batch is 3.52342.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.861953\n",
      "After 43001 training step(s), loss on training batch is 28.4728.\n",
      "After 43001 training step(s), pure_loss on training batch is 49.7646.\n",
      "After 43001 training step(s), regulayzer on training batch is 3.59047.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.864198\n",
      "After 44001 training step(s), loss on training batch is 28.0486.\n",
      "After 44001 training step(s), pure_loss on training batch is 49.2485.\n",
      "After 44001 training step(s), regulayzer on training batch is 3.42431.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.861953\n",
      "After 45001 training step(s), loss on training batch is 26.6247.\n",
      "After 45001 training step(s), pure_loss on training batch is 46.4159.\n",
      "After 45001 training step(s), regulayzer on training batch is 3.4168.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.863075\n",
      "After 46001 training step(s), loss on training batch is 25.1061.\n",
      "After 46001 training step(s), pure_loss on training batch is 43.3687.\n",
      "After 46001 training step(s), regulayzer on training batch is 3.42176.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.859708\n",
      "After 47001 training step(s), loss on training batch is 8.3681.\n",
      "After 47001 training step(s), pure_loss on training batch is 9.96449.\n",
      "After 47001 training step(s), regulayzer on training batch is 3.38586.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.863075\n",
      "After 48001 training step(s), loss on training batch is 28.5996.\n",
      "After 48001 training step(s), pure_loss on training batch is 50.4483.\n",
      "After 48001 training step(s), regulayzer on training batch is 3.37546.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.864198\n",
      "After 49001 training step(s), loss on training batch is 27.7355.\n",
      "After 49001 training step(s), pure_loss on training batch is 48.7689.\n",
      "After 49001 training step(s), regulayzer on training batch is 3.35101.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.863075\n",
      "After 50001 training step(s), loss on training batch is 26.1033.\n",
      "After 50001 training step(s), pure_loss on training batch is 45.3775.\n",
      "After 50001 training step(s), regulayzer on training batch is 3.41449.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.863075\n",
      "After 51001 training step(s), loss on training batch is 21.4022.\n",
      "After 51001 training step(s), pure_loss on training batch is 36.1191.\n",
      "After 51001 training step(s), regulayzer on training batch is 3.3427.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.863075\n",
      "After 52001 training step(s), loss on training batch is 26.4498.\n",
      "After 52001 training step(s), pure_loss on training batch is 46.2661.\n",
      "After 52001 training step(s), regulayzer on training batch is 3.31676.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.863075\n",
      "After 53001 training step(s), loss on training batch is 25.5339.\n",
      "After 53001 training step(s), pure_loss on training batch is 44.4861.\n",
      "After 53001 training step(s), regulayzer on training batch is 3.29086.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.863075\n",
      "After 54001 training step(s), loss on training batch is 26.2655.\n",
      "After 54001 training step(s), pure_loss on training batch is 45.8688.\n",
      "After 54001 training step(s), regulayzer on training batch is 3.3311.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.865320\n",
      "After 55001 training step(s), loss on training batch is 24.2883.\n",
      "After 55001 training step(s), pure_loss on training batch is 42.0353.\n",
      "After 55001 training step(s), regulayzer on training batch is 3.27066.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.864198\n",
      "After 56001 training step(s), loss on training batch is 26.7031.\n",
      "After 56001 training step(s), pure_loss on training batch is 46.6472.\n",
      "After 56001 training step(s), regulayzer on training batch is 3.37953.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.865320\n",
      "After 57001 training step(s), loss on training batch is 25.8308.\n",
      "After 57001 training step(s), pure_loss on training batch is 45.1528.\n",
      "After 57001 training step(s), regulayzer on training batch is 3.25446.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.865320\n",
      "After 58001 training step(s), loss on training batch is 27.4133.\n",
      "After 58001 training step(s), pure_loss on training batch is 48.386.\n",
      "After 58001 training step(s), regulayzer on training batch is 3.2203.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.866442\n",
      "After 59001 training step(s), loss on training batch is 24.4511.\n",
      "After 59001 training step(s), pure_loss on training batch is 42.458.\n",
      "After 59001 training step(s), regulayzer on training batch is 3.22216.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.865320\n",
      "After 60001 training step(s), loss on training batch is 28.5214.\n",
      "After 60001 training step(s), pure_loss on training batch is 50.4992.\n",
      "After 60001 training step(s), regulayzer on training batch is 3.27184.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.866442\n",
      "After 61001 training step(s), loss on training batch is 28.8834.\n",
      "After 61001 training step(s), pure_loss on training batch is 51.3604.\n",
      "After 61001 training step(s), regulayzer on training batch is 3.20315.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.867565\n",
      "After 62001 training step(s), loss on training batch is 27.2837.\n",
      "After 62001 training step(s), pure_loss on training batch is 48.2003.\n",
      "After 62001 training step(s), regulayzer on training batch is 3.18352.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.867565\n",
      "After 63001 training step(s), loss on training batch is 25.6951.\n",
      "After 63001 training step(s), pure_loss on training batch is 44.9009.\n",
      "After 63001 training step(s), regulayzer on training batch is 3.24462.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.864198\n",
      "After 64001 training step(s), loss on training batch is 14.467.\n",
      "After 64001 training step(s), pure_loss on training batch is 22.5797.\n",
      "After 64001 training step(s), regulayzer on training batch is 3.17713.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.865320\n",
      "After 65001 training step(s), loss on training batch is 26.5702.\n",
      "After 65001 training step(s), pure_loss on training batch is 46.8399.\n",
      "After 65001 training step(s), regulayzer on training batch is 3.15026.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.868687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 66001 training step(s), loss on training batch is 26.6356.\n",
      "After 66001 training step(s), pure_loss on training batch is 46.9777.\n",
      "After 66001 training step(s), regulayzer on training batch is 3.14676.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 67001 training step(s), loss on training batch is 25.4592.\n",
      "After 67001 training step(s), pure_loss on training batch is 44.4393.\n",
      "After 67001 training step(s), regulayzer on training batch is 3.23954.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.865320\n",
      "After 68001 training step(s), loss on training batch is 24.2014.\n",
      "After 68001 training step(s), pure_loss on training batch is 42.1779.\n",
      "After 68001 training step(s), regulayzer on training batch is 3.11244.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.865320\n",
      "After 69001 training step(s), loss on training batch is 25.6235.\n",
      "After 69001 training step(s), pure_loss on training batch is 44.7222.\n",
      "After 69001 training step(s), regulayzer on training batch is 3.26239.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.868687\n",
      "After 70001 training step(s), loss on training batch is 23.6264.\n",
      "After 70001 training step(s), pure_loss on training batch is 41.053.\n",
      "After 70001 training step(s), regulayzer on training batch is 3.09993.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 71001 training step(s), loss on training batch is 25.1728.\n",
      "After 71001 training step(s), pure_loss on training batch is 44.1292.\n",
      "After 71001 training step(s), regulayzer on training batch is 3.10823.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.868687\n",
      "After 72001 training step(s), loss on training batch is 23.4069.\n",
      "After 72001 training step(s), pure_loss on training batch is 40.55.\n",
      "After 72001 training step(s), regulayzer on training batch is 3.13183.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.867565\n",
      "After 73001 training step(s), loss on training batch is 27.321.\n",
      "After 73001 training step(s), pure_loss on training batch is 48.3312.\n",
      "After 73001 training step(s), regulayzer on training batch is 3.15537.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 74001 training step(s), loss on training batch is 27.5494.\n",
      "After 74001 training step(s), pure_loss on training batch is 48.9689.\n",
      "After 74001 training step(s), regulayzer on training batch is 3.06493.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 75001 training step(s), loss on training batch is 26.0623.\n",
      "After 75001 training step(s), pure_loss on training batch is 46.0078.\n",
      "After 75001 training step(s), regulayzer on training batch is 3.05841.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 76001 training step(s), loss on training batch is 24.4588.\n",
      "After 76001 training step(s), pure_loss on training batch is 42.7905.\n",
      "After 76001 training step(s), regulayzer on training batch is 3.0636.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.866442\n",
      "After 77001 training step(s), loss on training batch is 7.2734.\n",
      "After 77001 training step(s), pure_loss on training batch is 8.44661.\n",
      "After 77001 training step(s), regulayzer on training batch is 3.0501.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.868687\n",
      "After 78001 training step(s), loss on training batch is 28.6922.\n",
      "After 78001 training step(s), pure_loss on training batch is 51.3044.\n",
      "After 78001 training step(s), regulayzer on training batch is 3.03996.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.868687\n",
      "After 79001 training step(s), loss on training batch is 26.7156.\n",
      "After 79001 training step(s), pure_loss on training batch is 47.38.\n",
      "After 79001 training step(s), regulayzer on training batch is 3.02562.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 80001 training step(s), loss on training batch is 25.6865.\n",
      "After 80001 training step(s), pure_loss on training batch is 45.217.\n",
      "After 80001 training step(s), regulayzer on training batch is 3.07803.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.868687\n",
      "After 81001 training step(s), loss on training batch is 20.1404.\n",
      "After 81001 training step(s), pure_loss on training batch is 34.2166.\n",
      "After 81001 training step(s), regulayzer on training batch is 3.03204.\n",
      "Acc on val is 0.890110\n",
      "Acc on test is 0.869809\n",
      "After 82001 training step(s), loss on training batch is 26.4143.\n",
      "After 82001 training step(s), pure_loss on training batch is 46.8293.\n",
      "After 82001 training step(s), regulayzer on training batch is 2.99963.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 83001 training step(s), loss on training batch is 25.1718.\n",
      "After 83001 training step(s), pure_loss on training batch is 44.3509.\n",
      "After 83001 training step(s), regulayzer on training batch is 2.99631.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 84001 training step(s), loss on training batch is 25.7986.\n",
      "After 84001 training step(s), pure_loss on training batch is 45.5385.\n",
      "After 84001 training step(s), regulayzer on training batch is 3.02939.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.868687\n",
      "After 85001 training step(s), loss on training batch is 23.7093.\n",
      "After 85001 training step(s), pure_loss on training batch is 41.4554.\n",
      "After 85001 training step(s), regulayzer on training batch is 2.98161.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 86001 training step(s), loss on training batch is 26.1011.\n",
      "After 86001 training step(s), pure_loss on training batch is 46.0898.\n",
      "After 86001 training step(s), regulayzer on training batch is 3.05621.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 87001 training step(s), loss on training batch is 25.4826.\n",
      "After 87001 training step(s), pure_loss on training batch is 45.0142.\n",
      "After 87001 training step(s), regulayzer on training batch is 2.97552.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 88001 training step(s), loss on training batch is 26.8434.\n",
      "After 88001 training step(s), pure_loss on training batch is 47.752.\n",
      "After 88001 training step(s), regulayzer on training batch is 2.96745.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 89001 training step(s), loss on training batch is 23.933.\n",
      "After 89001 training step(s), pure_loss on training batch is 41.9213.\n",
      "After 89001 training step(s), regulayzer on training batch is 2.97234.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 90001 training step(s), loss on training batch is 28.2332.\n",
      "After 90001 training step(s), pure_loss on training batch is 50.5195.\n",
      "After 90001 training step(s), regulayzer on training batch is 2.97349.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 91001 training step(s), loss on training batch is 28.534.\n",
      "After 91001 training step(s), pure_loss on training batch is 51.178.\n",
      "After 91001 training step(s), regulayzer on training batch is 2.94501.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 92001 training step(s), loss on training batch is 26.5801.\n",
      "After 92001 training step(s), pure_loss on training batch is 47.2767.\n",
      "After 92001 training step(s), regulayzer on training batch is 2.94172.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.872054\n",
      "After 93001 training step(s), loss on training batch is 25.2461.\n",
      "After 93001 training step(s), pure_loss on training batch is 44.5547.\n",
      "After 93001 training step(s), regulayzer on training batch is 2.96878.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 94001 training step(s), loss on training batch is 12.9478.\n",
      "After 94001 training step(s), pure_loss on training batch is 20.0343.\n",
      "After 94001 training step(s), regulayzer on training batch is 2.93064.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n",
      "After 95001 training step(s), loss on training batch is 26.3187.\n",
      "After 95001 training step(s), pure_loss on training batch is 46.7999.\n",
      "After 95001 training step(s), regulayzer on training batch is 2.91872.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 96001 training step(s), loss on training batch is 26.1656.\n",
      "After 96001 training step(s), pure_loss on training batch is 46.4948.\n",
      "After 96001 training step(s), regulayzer on training batch is 2.91823.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.872054\n",
      "After 97001 training step(s), loss on training batch is 25.6536.\n",
      "After 97001 training step(s), pure_loss on training batch is 45.383.\n",
      "After 97001 training step(s), regulayzer on training batch is 2.96213.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n",
      "After 98001 training step(s), loss on training batch is 23.8757.\n",
      "After 98001 training step(s), pure_loss on training batch is 41.9538.\n",
      "After 98001 training step(s), regulayzer on training batch is 2.89877.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.870932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 99001 training step(s), loss on training batch is 25.3089.\n",
      "After 99001 training step(s), pure_loss on training batch is 44.6247.\n",
      "After 99001 training step(s), regulayzer on training batch is 2.99659.\n",
      "Acc on val is 0.879121\n",
      "Acc on test is 0.869809\n"
     ]
    }
   ],
   "source": [
    "# 初始化计算图\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.999\n",
    "REGULARAZTION_RATE = 0.07#正则化率\n",
    "TRAINING_STEPS = 100000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "PURE_LOSS_WIGHT = 0.5\n",
    "L1_OVER_L1ADDL2 = 0.6\n",
    "\n",
    "try:\n",
    "    tf.reset_default_graph()\n",
    "except:\n",
    "    pass\n",
    "train([X,Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
